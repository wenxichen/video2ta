# Video to Tutorial Assistant Chatbot

This is an simplified attempt to generate a structured chatbot from a video. Inspired by the [tweet](https://x.com/karpathy/status/1760740503614836917?s=20) from Karpathy and the [response](https://x.com/mlpowered/status/1764718705991442622?s=20) from Ameisen, the goal is to help educators generate a TA to help students review concepts in video. 

I am currently focus on educational videos. The current example used is [this](https://youtu.be/vStJoetOxJg?feature=shared) introductory machine learning video by Andrew Ng.

A simple example TA chatbot is generated on the [Juji](https://juji.io/) platform.

![generated tutorial step with frame image](img/juji_screenshot_1.png?raw=true "Screenshot of generated tutorial step with frame image")

Prompt to find best image go with the video on [Claude 3](https://console.anthropic.com).
```
Human: Here is a transcript of a video. The transcript was generated by an AI speech recognition tool and may contain some errors/infelicities. Also given the table of the contents and concepts found in this video. Can you recommend a frame to best go with the concept "Learning and Career Opportunities in Machine Learning" in a tutorial of this concept? Please just give the timestamp (round to second) of the frame.

Transcript
"""
{transcript}
"""

{table of contents}

{concept list}
```

Example REST API body to generate text base on images and text input (based on Ameisen's [promp](https://github.com/hundredblocks/transcription_demo)).
```
{
    "model": "claude-3-opus-20240229",
    "max_tokens": 1024,
    "messages": [
        {"role": "user", "content": [
            {"type": "text", "text": "Human: Here is a transcript of a video, including screenshots at different timestamps. The transcript was generated by an AI speech recognition tool and may contain some errors/infelicities. Your task is to transform the transcript into an html blog. The writing style of the blog, including desired balance between text and code is illustrated in a screenshot in <desired_writing_style>. The visual style of the blog, including page layout, font, headings and image styles are described in <desired_visual_style>.\n\nintro: Tokenization, GPT-2 paper, tokenization-related issues\n0:00\nhi everyone so in this video I'd like us to cover the process of tokenization in large language models now you see here\n0:06\nthat I have a set face and that's because uh tokenization is my least favorite part of working with large\n0:11\nlanguage models but unfortunately it is necessary to understand in some detail because it it is fairly hairy gnarly and\n0:17\nthere's a lot of hidden foot guns to be aware of and a lot of oddness with large language models typically traces back to\n0:24\ntokenization so what is tokenization now in my previous video Let's Build GPT from scratch uh we\n0:31\nactually already did tokenization but we did a very naive simple version of tokenization so when you go to the\n0:37\nGoogle colab for that video uh you see here that we loaded our training set and\n0:43\nour training set was this uh Shakespeare uh data set now in the beginning the Shakespeare data set is just a large\n0:49\nstring in Python it's just text and so the question is how do we plug text into\n0:54\nlarge language models and in this case here we created a vocabulary of 65\n1:01\npossible characters that we saw occur in this string these were the possible characters and we saw that there are 65\n1:07\nof them and then we created a a lookup table for converting from every possible\n1:13\ncharacter a little string piece into a token an integer so here for example we tokenized\n1:20\nthe string High there and we received this sequence of tokens and here we took the first 1,000\n1:27\ncharacters of our data set and we encoded it into tokens and because it is this is character level we received\n1:34\n1,000 tokens in a sequence so token 18 47\n1:40\nEtc now later we saw that the way we plug these tokens into the language\n1:45\nmodel is by using an embedding table and so basically if we have 65\n1:51\npossible tokens then this embedding table is going to have 65 rows and roughly speaking we're taking the\n1:58\ninteger associated with every single sing Le token we're using that as a lookup into this table and we're\n2:04\nplucking out the corresponding row and this row is a uh is trainable parameters\n2:09\nthat we're going to train using back propagation and this is the vector that then feeds into the Transformer um and\n2:15\nthat's how the Transformer Ser of perceives every single token so here we had a very naive\n2:21\ntokenization process that was a character level tokenizer but in practice in state-ofthe-art uh language\n2:27\nmodels people use a lot more complicated schemes unfortunately uh for constructing these uh token\n2:34\nvocabularies so we're not dealing on the Character level we're dealing on chunk level and the way these um character\n2:41\nchunks are constructed is using algorithms such as for example the bik pair in coding algorithm which we're\n2:46\ngoing to go into in detail um and cover in this video I'd like to briefly show\n2:52\nyou the paper that introduced a bite level encoding as a mechanism for tokenization in the context of large\n2:58\nlanguage models and I would say that that's probably the gpt2 paper and if you scroll down here to the section\n3:05\ninput representation this is where they cover tokenization the kinds of properties that you'd like the tokenization to have and they conclude\n3:13\nhere that they're going to have a tokenizer where you have a vocabulary of 50,2 57 possible\n3:20\ntokens and the context size is going to be 1,24 tokens so in the in in the\n3:27\nattention layer of the Transformer neural network every single token is attending to the previous tokens in the sequence and it's\n3:34\ngoing to see up to 1,24 tokens so tokens are this like fundamental unit um the\n3:40\natom of uh large language models if you will and everything is in units of tokens everything is about tokens and\n3:47\ntokenization is the process for translating strings or text into sequences of tokens and uh vice versa\n3:54\nwhen you go into the Llama 2 paper as well I can show you that when you search token you're going to get get 63 hits um\n4:01\nand that's because tokens are again pervasive so here they mentioned that they trained on two trillion tokens of\n4:06\ndata and so on so we're going to build our own tokenizer luckily the bite be encoding\n4:13\nalgorithm is not uh that super complicated and we can build it from scratch ourselves and we'll see exactly\n4:18\nhow this works before we dive into code I'd like to give you a brief Taste of some of the complexities that come from\n4:24\nthe tokenization because I just want to make sure that we motivate it sufficiently for why we are doing all\n4:29\nthis and why this is so gross so tokenization is at the heart of a lot of weirdness in large language models and I\n4:36\nwould advise that you do not brush it off a lot of the issues that may look like just issues with the new network\n4:42\narchitecture or the large language model itself are actually issues with the tokenization and fundamentally Trace uh\n4:49\nback to it so if you've noticed any issues with large language models can't\n4:54\nyou know not able to do spelling tasks very easily that's usually due to tokenization simple string processing\n5:00\ncan be difficult for the large language model to perform natively uh non-english languages can\n5:06\nwork much worse and to a large extent this is due to tokenization sometimes llms are bad at\n5:11\nsimple arithmetic also can trace be traced to tokenization uh gbt2 specifically would\n5:17\nhave had quite a bit more issues with python than uh future versions of it due to tokenization there's a lot of other\n5:24\nissues maybe you've seen weird warnings about a trailing whites space this is a tokenization issue um\n5:30\nif you had asked GPT earlier about solid gold Magikarp and what it is you would see the llm go totally crazy and it\n5:37\nwould start going off about a completely unrelated tangent topic maybe you've been told to use yl over Json in\n5:43\nstructure data all of that has to do with tokenization so basically tokenization is at the heart of many\n5:49\nissues I will look back around to these at the end of the video but for now let me just um skip over it a little bit and\ntokenization by example in a Web UI (tiktokenizer)\n5:56\nlet's go to this web app um the Tik tokenizer bell.app so I have it loaded\n6:02\nhere and what I like about this web app is that tokenization is running a sort of live in your browser in JavaScript so\n6:09\nyou can just type here stuff hello world and the whole string rokenes so here what we see on uh the\n6:18\nleft is a string that you put in on the right we're currently using the gpt2 tokenizer we see that this string that I\n6:24\npasted here is currently tokenizing into 300 tokens and here they are sort of uh\n6:30\nshown explicitly in different colors for every single token so for example uh this word tokenization became two tokens\n6:38\nthe token 3,642 and\n6:44\n1,634 the token um space is is token 318\n6:50\nso be careful on the bottom you can show white space and keep in mind that there are spaces and uh sln new line\n6:57\ncharacters in here but you can hide them for clarity the token space at is token 379\n7:06\nthe to the Token space the is 262 Etc so\n7:11\nyou notice here that the space is part of that uh token chunk now so this is kind of like how\n7:18\nour English sentence broke up and that seems all well and good now now here I\n7:24\nput in some arithmetic so we see that uh the token 127 Plus and then token six\n7:31\nspace 6 followed by 77 so what's happening here is that 127 is feeding in as a single token into the large\n7:38\nlanguage model but the um number 677 will actually feed in as two separate\n7:44\ntokens and so the large language model has to sort of um take account of that\n7:50\nand process it correctly in its Network and see here 804 will be broken up into\n7:56\ntwo tokens and it's is all completely arbitrary and here I have another example of four-digit numbers and they\n8:02\nbreak up in a way that they break up and it's totally arbitrary sometimes you have um multiple digits single token\n8:08\nsometimes you have individual digits as many tokens and it's all kind of pretty arbitrary and coming out of the\n8:14\ntokenizer here's another example we have the string egg and you see here that\n8:21\nthis became two tokens but for some reason when I say I have an egg you see when it's a space\n8:27\negg it's two token it's sorry it's a single token so just egg by itself in\n8:33\nthe beginning of a sentence is two tokens but here as a space egg is suddenly a single token uh for the exact\n8:40\nsame string okay here lowercase egg turns out to be a single token and in\n8:46\nparticular notice that the color is different so this is a different token so this is case sensitive and of course\n8:51\na capital egg would also be different tokens and again um this would be two\n8:57\ntokens arbitrarily so so for the same concept egg depending on if it's in the beginning of a sentence at the end of a\n9:03\nsentence lowercase uppercase or mixed all this will be uh basically very different tokens and different IDs and\n9:10\nthe language model has to learn from raw data from all the internet text that it's going to be training on that these are actually all the exact same concept\n9:17\nand it has to sort of group them in the parameters of the neural network and understand just based on the data\n9:22\npatterns that these are all very similar but maybe not almost exactly similar but but very very similar\n9:30\num after the EG demonstration here I have um an introduction from open a eyes\n9:35\nchbt in Korean so manaso Pang uh Etc uh\n9:41\nso this is in Korean and the reason I put this here is because you'll notice\n9:47\nthat um non-english languages work slightly worse in Chachi part of this is\n9:54\nbecause of course the training data set for Chachi is much larger for English and for everything else but the same is\n9:59\ntrue not just for the large language model itself but also for the tokenizer so when we train the tokenizer we're\n10:05\ngoing to see that there's a training set as well and there's a lot more English than non-english and what ends up\n10:11\nhappening is that we're going to have a lot more longer tokens for\n10:16\nEnglish so how do I put this if you have a single sentence in English and you tokenize it you might see that it's 10\n10:23\ntokens or something like that but if you translate that sentence into say Korean or Japanese or something else you'll\n10:29\ntypically see that the number of tokens used is much larger and that's because the chunks here are a lot more broken up\n10:36\nso we're using a lot more tokens for the exact same thing and what this does is it bloats up the sequence length of all\n10:43\nthe documents so you're using up more tokens and then in the attention of the Transformer when these tokens try to\n10:49\nattend each other you are running out of context um in the maximum context length\n10:55\nof that Transformer and so basically all the non-english text is stretched out\n11:01\nfrom the perspective of the Transformer and this just has to do with the um trainings that used for the tokenizer\n11:07\nand the tokenization itself so it will create a lot bigger tokens and a lot larger groups in English and it will\n11:14\nhave a lot of little boundaries for all the other non-english text um so if we\n11:19\ntranslated this into English it would be significantly fewer tokens the final example I have here is\n11:25\na little snippet of python for doing FS buuz and what I'd like you to notice is\n11:31\nlook all these individual spaces are all separate tokens they are token\n11:37\n220 so uh 220 220 220 220 and then space\n11:42\nif is a single token and so what's going on here is that when the Transformer is going to consume or try to uh create\n11:49\nthis text it needs to um handle all these spaces individually they all feed\n11:54\nin one by one into the entire Transformer in the sequence and so this is being extremely wasteful tokenizing\n\n<desired_visual_style>\n"},
            {
                "type": "image",
                "source": {
                    "type": "base64",
                    "media_type": "image/jpeg",
                    "data":"<image in base64>"
                }
            },
            {"type": "text", "text": "\n</desired_visual_style>\n\n<desired_writing_style>\n"},
            {
                "type": "image",
                "source": {
                    "type": "base64",
                    "media_type": "image/jpeg",
                    "data":"<image in base64>"
                },
                {"type": "text", "text": "\n</desired_writing_style>\n\nThis transcript is noisy. Please rewrite it into an html format for an blog using the following guidelines:\n- output valid html\n- insert section headings and other formatting where appropriate\n- use styling to make images, text, code, callouts and the page layout and margins look like the example in <desired_visual_style>\n- remove any verbal tics\n- if there are redundant pieces of information, only present it once\n- rewrite conversational content in the style shown in <desired_writing_style>, including headings to make the narrative structure easier to follow along\n- each transcript includes too many images, so you should only include the most important 1-2 images in your output\n- choose images that provide illustrations that are relevant to the transcript\n- prefer to include images which display complete code, rather than in progress\n- when relevant transcribe important pieces of code and other valuable text\n- if an image would help illustrate a part of a transcript, include it\n- to include an image, insert a tag  with src=\"frames/hh_mm_ss.jpg\"  (ie \"frames/00_12_35.jpg\") copying the exact image timestamp inserted above the image in the transcript\n- add captions to images\n- do not add any extraneous information: only include what is either mentioned in the transcript or the images\n\nYour final output should be suitable for inclusion in a textbook.\n\nAssistant: <!DOCTYPE html>"}
            ]
        }
    ]
}
```

## Future Work
- test longer videos and more cases
- integrate with RAG in the chatbot
- give reference to video timestamp in tutorial
- use LLM to select better image/frame for each tutorial
- include image in RAG
  - image understanding
  - present related image in response
- generation from textbook with image and text (e.g., [Solar Layout Analyzer](https://console.upstage.ai/playground/la))